{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beguiling West Niling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Jupyter Notebook containing all the code required to complete the Kaggle West Nile Virus Challenge. All directorial references can be fulfilled by downloading the repository and maintaining folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('../input/raw/weather.csv')\n",
    "spray = pd.read_csv('../input/raw/spray.csv')\n",
    "train = pd.read_csv('../input/raw/train.csv')\n",
    "test = pd.read_csv('../input/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Date to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Date = pd.to_datetime(train.Date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting rid of leakage (duplicate rows when NumMosquitos is greater than 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.groupby(['Date', 'Address', 'Species', 'Block', 'Street', 'Trap', 'AddressNumberAndStreet', 'Latitude', 'Longitude', 'AddressAccuracy', 'WnvPresent'])['NumMosquitos'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping geographical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['Address', 'Block', 'Street', 'AddressNumberAndStreet'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that Species and Trap are formatted as Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Species = train.Species.astype(str)\n",
    "train.Trap = train.Trap.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_spray = {\n",
    "    'Latitude': 'Lat',\n",
    "    'Longitude': 'Long'\n",
    "}\n",
    "\n",
    "train = train.rename(index=str, columns=rename_spray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('../input/clean/train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Date to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.Date = pd.to_datetime(test.Date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping geographical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop(['Address', 'Block', 'Street', 'AddressNumberAndStreet'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure that Species and Trap are formatted as Strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.Species = test.Species.astype(str)\n",
    "test.Trap = test.Trap.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_spray = {\n",
    "    'Latitude': 'Lat',\n",
    "    'Longitude': 'Long'\n",
    "}\n",
    "\n",
    "test = test.rename(index=str, columns=rename_spray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv('../input/clean/test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that allows as to take all numbers and convert them from string to numerical format.\n",
    "\n",
    "We're trying to convert everything stored as '10.0' to 10.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_float(x):\n",
    "    \n",
    "    x = x.strip()\n",
    "    \n",
    "    if x.isnumeric():\n",
    "        return float(x)\n",
    "    \n",
    "    if x == 'M' or x == 'T' or x == '-':\n",
    "        return None\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Date to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.Date = pd.to_datetime(weather.Date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the relevant columns to numeric formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_int = ['Station', 'Tmax', 'Tmin', 'Tavg', 'Depart', 'DewPoint', 'WetBulb', 'Heat', \\\n",
    "                  'Cool', 'Sunrise', 'Sunset', 'Depth', 'SnowFall', 'PrecipTotal', 'StnPressure', \\\n",
    "                  'SeaLevel', 'ResultSpeed', 'ResultDir', 'AvgSpeed']\n",
    "\n",
    "for col in columns_to_int:\n",
    "    try:\n",
    "        weather[col] = weather[col].astype('float')\n",
    "    except:\n",
    "        weather[col] = weather[col].apply(make_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather = weather.drop(['Water1'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename_weather = {\n",
    "    'Tmax': 'Temp_Max',\n",
    "    'Tmin': 'Temp_Min',\n",
    "    'Tavg': 'Temp_Avg',\n",
    "    'Depart': 'Temp_Norm_Dev',\n",
    "    'DewPoint': 'Temp_Dew_Point',\n",
    "    'WetBulb': 'Temp_Wet_Bulb',\n",
    "    'Depth': 'Max_Snow_Depth',\n",
    "}\n",
    "\n",
    "weather = weather.rename(index=str, columns=rename_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to a new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.to_csv('../input/clean/weather_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Spray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray = spray.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray = spray.drop(['Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rename_spray = {\n",
    "    'Latitude': 'Lat',\n",
    "    'Longitude': 'Long'\n",
    "}\n",
    "\n",
    "spray = spray.rename(index=str, columns=rename_spray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.to_csv('../input/clean/spray_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/clean/train_cleaned.csv').drop('Unnamed: 0', axis=1).dropna(axis=1)\n",
    "test = pd.read_csv('../input/clean/test_cleaned.csv').drop(['Unnamed: 0', 'Id'], axis=1).dropna(axis=1)\n",
    "weather = pd.read_csv('../input/clean/weather_cleaned.csv').drop('Unnamed: 0', axis=1)\n",
    "spray = pd.read_csv('../input/clean/spray_cleaned.csv').drop('Unnamed: 0', axis=1).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Date = pd.to_datetime(train.Date, infer_datetime_format=True)\n",
    "test.Date = pd.to_datetime(test.Date, infer_datetime_format=True)\n",
    "weather.Date = pd.to_datetime(weather.Date, infer_datetime_format=True)\n",
    "spray.Date = pd.to_datetime(spray.Date, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spray.groupby([spray[\"Date\"].dt.year, spray[\"Date\"].dt.week]).count().plot(kind=\"bar\",figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a duplicated rows feature to address leakage in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Duplicated_Rows'] = train.NumMosquitos.apply(lambda x : round(x / 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the same duplicated rows feature in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Boolean column that simply says if a row is Duplicated\n",
    "test['Duplicated'] = test.duplicated(['Date', 'Species', 'Trap', 'Lat', 'Long', 'AddressAccuracy'])\n",
    "\n",
    "## Generating an empty column\n",
    "test['Duplicated_Rows'] = test['Duplicated'].apply(lambda x : 1 if x else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that determines how many rows are duplicated in test data for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_duplicated(row):\n",
    "    \n",
    "    size = 0\n",
    "    \n",
    "    if row.Duplicated:\n",
    "        size = test[(test.Date == row.Date) & (test.Species == row.Species) \\\n",
    "                    & (test.Trap == row.Trap) & (test.Lat == row.Lat) \\\n",
    "                    & (test.Long == row.Long)].shape[0] - 1\n",
    "    \n",
    "    return size\n",
    "\n",
    "test['Duplicated_Rows'] = test.apply(fill_duplicated, axis=1)\n",
    "\n",
    "## Dropping boolean column\n",
    "test = test.drop('Duplicated', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a trap dictionary in order to factorize trap consistently over both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trap_dict = {}\n",
    "\n",
    "for i, trap in enumerate(list(test.Trap.unique())):\n",
    "    trap_dict[trap]= i + 1\n",
    "    \n",
    "train['Trap_Fact'] = train['Trap'].apply(lambda x : trap_dict[x])\n",
    "test['Trap_Fact'] = test['Trap'].apply(lambda x : trap_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Date to an ordinal feature and creating columns based on date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "train['Date_Ord'] = train.Date.apply(datetime.date.toordinal)\n",
    "test['Date_Ord'] = test.Date.apply(datetime.date.toordinal)\n",
    "\n",
    "train['Week'] = train.Date.dt.week\n",
    "train['Year'] = train.Date.dt.year\n",
    "\n",
    "test['Week'] = test.Date.dt.week\n",
    "test['Year'] = test.Date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating days since last checked a trap. This is a necessary feature because number of mosquitos is proportional to how long a trap hasn't been checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for converting nano-seconds to days (created to deal with Numpy default datetime settings).\n",
    "\n",
    "def nanosec2days(diff):\n",
    "    return int(diff / ((10 ** 9) * 60 * 60 * 24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every trap, for every day, how many days since last checked. For training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_trap_diffs = {}\n",
    "\n",
    "for trap in train.Trap.unique():\n",
    "#     print(trap)\n",
    "    df_trap = train[train.Trap == trap]\n",
    "    \n",
    "    diff_dict = {}\n",
    "    uniq_dates = df_trap.Date.unique()\n",
    "    for i, date in enumerate(uniq_dates):\n",
    "        if not i:\n",
    "            diff_dict[str(date)[:10]] = 0\n",
    "        else:\n",
    "            diff_dict[str(date)[:10]] = nanosec2days(uniq_dates[i] - uniq_dates[i - 1])\n",
    "\n",
    "    train_trap_diffs[trap] = diff_dict\n",
    "    \n",
    "train_trap_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_trap_diffs = {}\n",
    "\n",
    "for trap in test.Trap.unique():\n",
    "#     print(trap)\n",
    "    df_trap = test[test.Trap == trap]\n",
    "    \n",
    "    diff_dict = {}\n",
    "    uniq_dates = df_trap.Date.unique()\n",
    "    for i, date in enumerate(uniq_dates):\n",
    "        if not i:\n",
    "            diff_dict[str(date)[:10]] = 0\n",
    "        else:\n",
    "            diff_dict[str(date)[:10]] = nanosec2days(uniq_dates[i] - uniq_dates[i - 1])\n",
    "\n",
    "    test_trap_diffs[trap] = diff_dict\n",
    "    \n",
    "test_trap_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fill 'Days since Last Checked' for test and train data. Created multiple functions because I'm using DataFrame.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_train_diffs(row):\n",
    "    \n",
    "    year = str(row.Date.year)\n",
    "    month = str(row.Date.month)\n",
    "    day = str(row.Date.day)\n",
    "    \n",
    "    if len(month) == 1:\n",
    "        month = '0' + month\n",
    "    if len(day) == 1:\n",
    "        day = '0' + day\n",
    "    \n",
    "    date = year + '-' + month + '-' + day\n",
    "#     print(date)\n",
    "    \n",
    "    return train_trap_diffs[row.Trap][date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_test_diffs(row):\n",
    "    \n",
    "    year = str(row.Date.year)\n",
    "    month = str(row.Date.month)\n",
    "    day = str(row.Date.day)\n",
    "    \n",
    "    if len(month) == 1:\n",
    "        month = '0' + month\n",
    "    if len(day) == 1:\n",
    "        day = '0' + day\n",
    "    \n",
    "    date = year + '-' + month + '-' + day\n",
    "#     print(date)\n",
    "    \n",
    "    return test_trap_diffs[row.Trap][date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Days_Since_Last_Check'] = train.apply(fill_train_diffs, axis=1)\n",
    "test['Days_Since_Last_Check'] = test.apply(fill_test_diffs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Species and Trap columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['Species', 'Trap'], axis=1)\n",
    "test = test.drop(['Species', 'Trap'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a function to calculate Spray Factor for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spray_factor(row):\n",
    "    \n",
    "    Date = row.Date\n",
    "    Lat = row.Lat\n",
    "    Long = row.Long   \n",
    "    \n",
    "    rel = spray[(spray['Date'] < Date) & ((Date-spray['Date']) < pd.Timedelta('14 days'))]\n",
    "    \n",
    "    factor = 0\n",
    "    for item in rel.iterrows():\n",
    "        latdiff = Lat - item[1]['Lat']\n",
    "        longdiff = Long - item[1]['Long']\n",
    "        dis = .5 ** (latdiff ** 2 + longdiff ** 2)\n",
    "        t = (Date - item[1]['Date']).days\n",
    "        factor += (.5) ** (t / 1.5) / dis\n",
    "    return factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Spray_Factor'] = train.apply(spray_factor, axis=1)\n",
    "test['Spray_Factor'] = test.apply(spray_factor, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummifying weather codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes = []\n",
    "for code in [item.split() for item in weather.CodeSum.unique()]:\n",
    "    codes += code\n",
    "\n",
    "codes = list(set(codes))\n",
    "codes\n",
    "\n",
    "for code in codes:\n",
    "    weather[code] = weather.CodeSum.apply(lambda x : 1 if code in x else 0)\n",
    "   \n",
    " ## Dropping CodeSum Column\n",
    "weather = weather.drop('CodeSum', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that calculates length of day (will be used further down in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_diff(row):\n",
    "    \n",
    "    try:\n",
    "        sunset = (round(row.Sunset / 100) * 60) + (row.Sunset % 100)\n",
    "        sunrise = (round(row.Sunrise / 100) * 60) + (row.Sunrise % 100)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    return int(abs(sunset - sunrise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to extrapolate average weather across stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weather_extrap(colname):\n",
    "    \n",
    "    for i in range(len(weather_both[colname])):\n",
    "        \n",
    "          if np.isnan(weather_both[colname][i]):\n",
    "                \n",
    "                if not np.isnan(weather1[colname][i]):\n",
    "                    weather_both[colname][i] = weather1[colname][i]\n",
    "                    \n",
    "                elif not np.isnan(weather2[colname][i]):\n",
    "                    weather_both[colname][i] = weather2[colname][i]\n",
    "                    \n",
    "                else:\n",
    "                    weather_both[colname][i] = 0.5 * (weather_both[colname][i - 1] + weather_both[colname][i + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating aggregate features across both stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Station', 'Date', 'Tmax', 'Tmin', 'Tavg', 'Depart',\n",
       "       'DewPoint', 'WetBulb', 'Heat', 'Cool', 'Sunrise', 'Sunset', 'Depth',\n",
       "       'SnowFall', 'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed',\n",
       "       'ResultDir', 'AvgSpeed', 'GR', 'BR', 'SQ', 'FU', 'RA', 'TSRA', 'MIFG',\n",
       "       'VCFG', 'BCFG', 'SN', 'VCTS', 'DZ', 'HZ', 'FG+', 'FG', 'TS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_both = pd.DataFrame()\n",
    "\n",
    "weather1 = weather[weather['Station'] == 1].reset_index()\n",
    "weather2 = weather[weather['Station'] == 2].reset_index()\n",
    "\n",
    "weather_both['Date'] = weather1['Date']\n",
    "weather_both[codes] = weather1[codes]\n",
    "weather_both['Temp_Min_both']=.5*(weather1['Tmin'] + weather2['Tmin'])\n",
    "weather_both['Temp_Max_both']=.5*(weather1['Tmax'] + weather2['Tmax'])\n",
    "weather_both['Temp_Avg_both']=.5*(weather1['Tavg'] + weather2['Tavg'])\n",
    "weather_both['Temp_Dew_Point_both']=.5*(weather1['DewPoint'] + weather2['DewPoint'])\n",
    "weather_both['Temp_Wet_Bulb_both']=.5*(weather1['WetBulb'] + weather2['WetBulb'])\n",
    "weather_both['StnPressure_both']=.5*(weather1['StnPressure'] + weather2['StnPressure'])\n",
    "weather_both['Ht_Cool_both']= weather1['Heat'] - weather1['Cool']\n",
    "weather_both['Day Length'] = weather1.apply(time_diff, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging weather data on Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(weather_both, on='Date')\n",
    "test = test.merge(weather_both, on='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Number of Mosquitos from train data and Date from both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['NumMosquitos', 'Date'], axis=1)\n",
    "test = test.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating aggregated columns with time lagged data for various functions.\n",
    "\n",
    "This allows us to see information like 'Maximum temperature over last 14 days' or 'Average humidity over last 3 days'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_agg_cols(colname, func, deltas):\n",
    "    new_frame=pd.DataFrame()\n",
    "    base_col=weather_both[colname]\n",
    "\n",
    "    for delta in deltas:\n",
    "        new_colname=colname+str(delta)\n",
    "        new_frame[new_colname]=[0]*len(base_col)\n",
    "\n",
    "        for i in range(delta,len(base_col)):\n",
    "            relevant=base_col[i-delta: i]\n",
    "            new_frame[new_colname][i]=func(relevant)\n",
    "\n",
    "    return new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deltas=[1,2,4,7,10,14]\n",
    "\n",
    "weather_agged = weather_both\n",
    "\n",
    "weather_agged = pd.concat([weather_agged, create_agg_cols('Temp_Min_both', min, deltas)], axis=1)\n",
    "weather_agged = pd.concat([weather_agged, create_agg_cols('Temp_Max_both', max, deltas)], axis=1)\n",
    "weather_agged = pd.concat([weather_agged, create_agg_cols('Temp_Dew_Point_both', max, deltas)], axis=1)\n",
    "weather_agged = pd.concat([weather_agged, create_agg_cols('Temp_Avg_both', np.mean, deltas)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging time lagged data on Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.merge(weather_agged, on='Date')\n",
    "test = test.merge(weather_agged, on='Date')\n",
    "\n",
    "## Dropping Date\n",
    "train = train.drop(['Date'], axis=1)\n",
    "test = test.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a count to show how often a specific Trap turns up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trap_count = train.groupby('Trap_Fact').count().Duplicated_Rows.to_dict()\n",
    "\n",
    "for key in list(set(test.groupby('Trap_Fact').count().Duplicated_Rows.to_dict()) - set(train.groupby('Trap_Fact').count().Duplicated_Rows.to_dict().keys())):\n",
    "    trap_count[key] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Trap_Count'] = train.Trap_Fact.apply(lambda x : trap_count[x])\n",
    "test['Trap_Count'] = test.Trap_Fact.apply(lambda x : trap_count[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a version of the last version trap count to show how often a specific Trap turns up in a year. These variables naturally have high correlation, but they do prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trap_by_year_count = train.groupby(['Year', 'Trap_Fact']).count().Duplicated_Rows.to_dict()\n",
    "\n",
    "for key in list(set(test.groupby('Trap_Fact').count().Duplicated_Rows.to_dict()) - set(train.groupby('Trap_Fact').count().Duplicated_Rows.to_dict().keys())):\n",
    "    trap_by_year_count[(None,key)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating trap_by_year_count for train.\n",
    "\n",
    "train['Trap_By_Year_Count'] = train.Year.apply(lambda x : 0)\n",
    "\n",
    "def fill_tbyc(row):\n",
    "    return trap_by_year_count[(row['Year'], row['Trap_Fact'])]\n",
    "    \n",
    "train['Trap_By_Year_Count'] = train.apply(fill_tbyc, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating trap_by_year_count for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['Trap_By_Year_Count'] = test.Year.apply(lambda x : 0)\n",
    "\n",
    "def fill_tbyc(row):\n",
    "    try:\n",
    "        count = trap_by_year_count[(row['Year'], row['Trap_Fact'])]\n",
    "    except:\n",
    "        try:\n",
    "            count = trap_by_year_count[(row['Year'] + 1, row['Trap_Fact'])]\n",
    "        except:\n",
    "            count = -1\n",
    "    return count\n",
    "    \n",
    "test['Trap_By_Year_Count'] = test.apply(fill_tbyc, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing empty values in WetBulb and StnPressure columns with -1 to make it usable in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_none(x):\n",
    "    if x == 'NaN':\n",
    "        return -1\n",
    "    return x\n",
    "\n",
    "train.Temp_Wet_Bulb_both = train.Temp_Wet_Bulb_both.apply(replace_none)\n",
    "test.Temp_Wet_Bulb_both = test.Temp_Wet_Bulb_both.apply(replace_none)\n",
    "\n",
    "train.StnPressure_both = train.StnPressure_both.apply(replace_none)\n",
    "test.StnPressure_both = test.StnPressure_both.apply(replace_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train[['WnvPresent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping WnV Present from train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['WnvPresent'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Year columns - they are already represented in the ordinal dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['Year'], axis=1)\n",
    "test = test.drop(['Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to a final CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('../../../input/final/train_final.csv')\n",
    "target.to_csv('../../../input/final/train_target_final.csv')\n",
    "test.to_csv('../../../input/final/test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../../../input/final/train_final.csv').drop('Unnamed: 0', axis=1)\n",
    "target = pd.read_csv('../../../input/final/train_target_final.csv').drop('Unnamed: 0', axis=1).WnvPresent\n",
    "test = pd.read_csv('../../../input/final/test_final.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that allows us easily write a csv by just passing in a model and a model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv(model_name, model):\n",
    "    \n",
    "    model_data = [item[1] for item in model.predict_proba(test)]\n",
    "    \n",
    "    pd.DataFrame(data=model_data).to_csv(model_name + '.csv')\n",
    "    results = pd.read_csv(model_name + '.csv')\n",
    "    results.columns = ['Id', 'WnvPresent']\n",
    "    results.Id = results.Id.apply(lambda x : x + 1)\n",
    "    results.to_csv(model_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(learning_rate=0.2, seed=42, silent=1) # 0.2 best so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1.fit(train, target)\n",
    "\n",
    "# plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "plot_importance(xgb1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all columns with 0 feature importance. This should make no difference to the XGBoost model (apart from making things easier to read and more manageable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = dict(zip(train.columns, xgb1.feature_importances_ * 100 / max(xgb1.feature_importances_)))\n",
    "\n",
    "for key in importance.keys():\n",
    "    if importance[key] == 0:\n",
    "        train = train.drop([key], axis=1)\n",
    "        test = test.drop([key], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb1.fit(train, target)\n",
    "\n",
    "# plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 15))\n",
    "plot_importance(xgb1, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_csv('xgboost', xgb1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
